{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset contains Reviews of movies and a binary classification of the movies as output. The words of the reviews are indices of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franzi/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/franzi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/franzi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, Flatten\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=1000,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding of sentences\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train2 = to_categorical(y_train, num_classes)\n",
    "y_test2 = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 128,386\n",
      "Trainable params: 128,386\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 3s 154us/step - loss: 0.4403 - acc: 0.7927 - val_loss: 0.3357 - val_acc: 0.8580\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.3263 - acc: 0.8599 - val_loss: 0.3291 - val_acc: 0.8576\n",
      "25000/25000 [==============================] - 2s 80us/step\n",
      "('\\n Training Accuracy:', 0.88068)\n",
      "25000/25000 [==============================] - 2s 87us/step\n",
      "('\\n Testing Accuracy:', 0.85812)\n"
     ]
    }
   ],
   "source": [
    "# Neural Network model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train2, epochs=2, batch_size=100, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(x_train, y_train2)\n",
    "print(\"\\n Training Accuracy:\", score[1])\n",
    "score = model.evaluate(x_test, y_test2)\n",
    "print(\"\\n Testing Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 100)         100000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100000)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 100001    \n",
      "=================================================================\n",
      "Total params: 200,001\n",
      "Trainable params: 200,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 18s 725us/step - loss: 0.3885 - acc: 0.82601s - loss:\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 17s 661us/step - loss: 0.3268 - acc: 0.8634\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 17s 693us/step - loss: 0.3196 - acc: 0.8668\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 16s 655us/step - loss: 0.3165 - acc: 0.8696\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 18s 707us/step - loss: 0.3136 - acc: 0.8702\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 18s 738us/step - loss: 0.3139 - acc: 0.8698\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 17s 669us/step - loss: 0.3113 - acc: 0.8702\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 17s 672us/step - loss: 0.3117 - acc: 0.8719\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 16s 624us/step - loss: 0.3102 - acc: 0.8717\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 15s 607us/step - loss: 0.3106 - acc: 0.8706\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 15s 582us/step - loss: 0.3094 - acc: 0.8710\n",
      "Epoch 12/50\n",
      "25000/25000 [==============================] - 14s 573us/step - loss: 0.3081 - acc: 0.8728\n",
      "Epoch 13/50\n",
      "25000/25000 [==============================] - 16s 658us/step - loss: 0.3084 - acc: 0.8718\n",
      "Epoch 14/50\n",
      "25000/25000 [==============================] - 17s 682us/step - loss: 0.3079 - acc: 0.8729\n",
      "Epoch 15/50\n",
      "25000/25000 [==============================] - 15s 588us/step - loss: 0.3076 - acc: 0.8715\n",
      "Epoch 16/50\n",
      "25000/25000 [==============================] - 15s 616us/step - loss: 0.3066 - acc: 0.8724\n",
      "Epoch 17/50\n",
      "25000/25000 [==============================] - 16s 650us/step - loss: 0.3070 - acc: 0.8723\n",
      "Epoch 18/50\n",
      "25000/25000 [==============================] - 15s 601us/step - loss: 0.3065 - acc: 0.8718\n",
      "Epoch 19/50\n",
      "25000/25000 [==============================] - 15s 605us/step - loss: 0.3059 - acc: 0.8744\n",
      "Epoch 20/50\n",
      "25000/25000 [==============================] - 15s 616us/step - loss: 0.3056 - acc: 0.8739\n",
      "Epoch 21/50\n",
      "25000/25000 [==============================] - 16s 649us/step - loss: 0.3067 - acc: 0.8730\n",
      "Epoch 22/50\n",
      "25000/25000 [==============================] - 14s 578us/step - loss: 0.3055 - acc: 0.8726\n",
      "Epoch 23/50\n",
      "25000/25000 [==============================] - 15s 598us/step - loss: 0.3052 - acc: 0.8748\n",
      "Epoch 24/50\n",
      "25000/25000 [==============================] - 14s 548us/step - loss: 0.3056 - acc: 0.8739\n",
      "Epoch 25/50\n",
      "25000/25000 [==============================] - 14s 548us/step - loss: 0.3051 - acc: 0.8728\n",
      "Epoch 26/50\n",
      "25000/25000 [==============================] - 14s 546us/step - loss: 0.3061 - acc: 0.8736\n",
      "Epoch 27/50\n",
      "25000/25000 [==============================] - 14s 578us/step - loss: 0.3036 - acc: 0.8741\n",
      "Epoch 28/50\n",
      "25000/25000 [==============================] - 14s 544us/step - loss: 0.3045 - acc: 0.8736\n",
      "Epoch 29/50\n",
      "25000/25000 [==============================] - 14s 563us/step - loss: 0.3042 - acc: 0.8739\n",
      "Epoch 30/50\n",
      "25000/25000 [==============================] - 14s 561us/step - loss: 0.3044 - acc: 0.8740\n",
      "Epoch 31/50\n",
      "25000/25000 [==============================] - 15s 585us/step - loss: 0.3053 - acc: 0.8740\n",
      "Epoch 32/50\n",
      "25000/25000 [==============================] - 14s 548us/step - loss: 0.3042 - acc: 0.8730\n",
      "Epoch 33/50\n",
      "25000/25000 [==============================] - 14s 549us/step - loss: 0.3034 - acc: 0.8743\n",
      "Epoch 34/50\n",
      "25000/25000 [==============================] - 14s 547us/step - loss: 0.3029 - acc: 0.8756\n",
      "Epoch 35/50\n",
      "25000/25000 [==============================] - 14s 548us/step - loss: 0.3037 - acc: 0.8746\n",
      "Epoch 36/50\n",
      "25000/25000 [==============================] - 14s 548us/step - loss: 0.3040 - acc: 0.8737\n",
      "Epoch 37/50\n",
      "25000/25000 [==============================] - 14s 547us/step - loss: 0.3034 - acc: 0.8745\n",
      "Epoch 38/50\n",
      "25000/25000 [==============================] - 14s 562us/step - loss: 0.3038 - acc: 0.8752\n",
      "Epoch 39/50\n",
      "25000/25000 [==============================] - 14s 547us/step - loss: 0.3039 - acc: 0.8746\n",
      "Epoch 40/50\n",
      "25000/25000 [==============================] - 14s 544us/step - loss: 0.3035 - acc: 0.8755\n",
      "Epoch 41/50\n",
      "25000/25000 [==============================] - 14s 573us/step - loss: 0.3034 - acc: 0.8744\n",
      "Epoch 42/50\n",
      "25000/25000 [==============================] - 15s 592us/step - loss: 0.3030 - acc: 0.8751\n",
      "Epoch 43/50\n",
      "25000/25000 [==============================] - 15s 587us/step - loss: 0.3026 - acc: 0.8755\n",
      "Epoch 44/50\n",
      "25000/25000 [==============================] - 16s 634us/step - loss: 0.3028 - acc: 0.8741\n",
      "Epoch 45/50\n",
      "25000/25000 [==============================] - 15s 595us/step - loss: 0.3027 - acc: 0.8750\n",
      "Epoch 46/50\n",
      "25000/25000 [==============================] - 15s 609us/step - loss: 0.3021 - acc: 0.8750\n",
      "Epoch 47/50\n",
      "25000/25000 [==============================] - 15s 594us/step - loss: 0.3035 - acc: 0.8736\n",
      "Epoch 48/50\n",
      "25000/25000 [==============================] - 17s 690us/step - loss: 0.3020 - acc: 0.8756\n",
      "Epoch 49/50\n",
      "25000/25000 [==============================] - 16s 645us/step - loss: 0.3034 - acc: 0.8732\n",
      "Epoch 50/50\n",
      "25000/25000 [==============================] - 15s 604us/step - loss: 0.3030 - acc: 0.8755\n",
      "Accuracy: 85.932000\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(x_train.shape[1], 100, input_length=x_train.shape[1])\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(x_train, y_train, epochs=50)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
